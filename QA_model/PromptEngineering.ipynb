{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KB5dPi98moQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "def get_score(model, tokenizer, input_ids):\n",
        "    pos_ids = tokenizer('Yes', return_tensors='pt').input_ids\n",
        "    neg_ids = tokenizer('No', return_tensors='pt').input_ids\n",
        "    pos_id = pos_ids[0, 0]\n",
        "    neg_id = neg_ids[0, 0]\n",
        "\n",
        "    logits = model(input_ids, decoder_input_ids=torch.zeros((input_ids.size(0), 1), dtype=torch.long).to(model.device)).logits\n",
        "    pos_logits = logits[:, 0, pos_id]\n",
        "    neg_logits = logits[:, 0, neg_id]\n",
        "    posneg_logits = torch.cat([pos_logits.unsqueeze(-1), neg_logits.unsqueeze(-1)], dim=1)\n",
        "    scores = torch.nn.functional.softmax(posneg_logits, dim=1)[:, 0]\n",
        "\n",
        "    return scores\n",
        "\n",
        "entailment_pairs = [\n",
        "    (\"In his speech, the President emphasized the need for bipartisan cooperation to pass significant climate change legislation.\", \"The President highlighted the importance of working together across party lines to address climate change.\", True),\n",
        "    (\"The latest economic report shows a significant increase in job growth, especially in the technology and healthcare sectors.\", \"Job growth has risen notably, particularly in technology and healthcare according to the recent economic report.\", True),\n",
        "    (\"The Supreme Court ruled that the new voting regulations are unconstitutional, citing violations of the Fourteenth Amendment.\", \"The Supreme Court found the new voting laws unconstitutional due to conflicts with the Fourteenth Amendment.\", True),\n",
        "    (\"In his speech, the President emphasized the need for bipartisan cooperation to pass significant climate change legislation.\", \"The President announced new healthcare policies aimed at reducing drug prices.\", False),\n",
        "    (\"The latest economic report shows a significant increase in job growth, especially in the technology and healthcare sectors.\", \"The economic report indicates a downturn in the technology and healthcare sectors.\", False),\n",
        "    (\"The Supreme Court ruled that the new voting regulations are unconstitutional, citing violations of the Fourteenth Amendment.\", \"The Supreme Court upheld the new voting regulations as constitutional.\", False)\n",
        "]\n",
        "\n",
        "# Difficult question-answer pairs\n",
        "qa_pairs = [\n",
        "    (\"What did the President emphasize in his speech regarding climate change legislation?\", \"The President emphasized the need for bipartisan cooperation to pass significant climate change legislation.\", True),\n",
        "    (\"What did the President emphasize in his speech regarding climate change legislation?\", \"He emphasized it was not so good for you.\", False),\n",
        "    (\"Which sectors showed significant job growth in the latest economic report?\", \"The technology and healthcare sectors showed significant job growth in the latest economic report.\", True),\n",
        "    (\"What was the Supreme Court's ruling on the new voting regulations?\", \"The Supreme Court ruled that the new voting regulations are unconstitutional, citing violations of the Fourteenth Amendment.\", True),\n",
        "    (\"What did the President emphasize in his speech regarding climate change legislation?\", \"The President announced new healthcare policies aimed at reducing drug prices.\", False),\n",
        "    (\"Which sectors showed significant job growth in the latest economic report?\", \"The economic report indicates a downturn in the technology and healthcare sectors.\", False),\n",
        "    (\"What was the Supreme Court's ruling on the new voting regulations?\", \"The Supreme Court upheld the new voting regulations as constitutional.\", False)\n",
        "]\n",
        "\n",
        "\n",
        "# Generate entailment predictions and scores\n",
        "for premise, hypothesis, truth in entailment_pairs:\n",
        "    print(f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nTrue Entailment: {truth} \\n\")\n",
        "\n",
        "    prompt = f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nDoes the hypothesis follow from the premise?\\nAnswer:\"\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "    scores = get_score(model, tokenizer, input_ids)\n",
        "    print(f'Probability that the hypothesis follows from the premise: {scores[0].item()}')\n",
        "\n",
        "    prompt = f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nIs the hypothesis a logical conclusion based on the premise?\\nAnswer:\"\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "    scores = get_score(model, tokenizer, input_ids)\n",
        "    print(f'Probability that the hypothesis is a logical conclusion based on the premise: {scores[0].item()}')\n",
        "\n",
        "    prompt = f\"Sentence1: {premise}\\nSentence2: {hypothesis}\\nDo these sentences discuss the same event?\\nAnswer:\"\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "    scores = get_score(model, tokenizer, input_ids)\n",
        "    print(f'Probability that sentence2 discusses the same event as sentence1: {scores[0].item()}')\n",
        "\n",
        "    prompt_concrete = f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nAre the premise and hypothesis discussing the same topic?\\nAnswer:\"\n",
        "    input_ids = tokenizer(prompt_concrete, return_tensors='pt').input_ids\n",
        "    scores_concrete = get_score(model, tokenizer, input_ids)\n",
        "    print(f'Probability the premise and hypothesis are about the same topic: {scores_concrete[0].item()} \\n')\n",
        "\n",
        "# Generate scores for question-answer pairs\n",
        "print(\"Complex and Subjective Question-Answer Pairs:\")\n",
        "for question, answer, truth in qa_pairs:\n",
        "    print(f\"Question: {question}\\nAnswer: {answer}\\nTrue QA Pair: {truth}\")\n",
        "\n",
        "    # Check if it is a question-answer pair\n",
        "    prompt_qa = f\"Question: {question}\\nAnswer: {answer}\\nIs this a valid question-answer pair?\\nAnswer:\"\n",
        "    input_ids = tokenizer(prompt_qa, return_tensors='pt').input_ids\n",
        "    scores_qa = get_score(model, tokenizer, input_ids)\n",
        "    print(f'Probability that this is a valid question-answer pair: {scores_qa[0].item()} \\n')\n",
        "\n",
        "    # Check if the answer addresses the question\n",
        "    prompt = f\"Question: {question}\\nAnswer: {answer}\\nDoes this answer correctly respond to the question?\\nAnswer:\"\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "    scores_qa = get_score(model, tokenizer, input_ids)\n",
        "    print(f'Probability that this answer correctly responds to the question: {scores_qa[0].item()} \\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQvbsCyC8nAs",
        "outputId": "2f2ad071-3eb0-4705-f277-a2e4a5734b10"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Premise: In his speech, the President emphasized the need for bipartisan cooperation to pass significant climate change legislation.\n",
            "Hypothesis: The President highlighted the importance of working together across party lines to address climate change.\n",
            "True Entailment: True \n",
            "\n",
            "Probability that the hypothesis follows from the premise: 0.9891353845596313\n",
            "Probability that the hypothesis is a logical conclusion based on the premise: 0.9884614944458008\n",
            "Probability that sentence2 discusses the same event as sentence1: 0.9778763055801392\n",
            "Probability the premise and hypothesis are about the same topic: 0.9860095977783203 \n",
            "\n",
            "Premise: The latest economic report shows a significant increase in job growth, especially in the technology and healthcare sectors.\n",
            "Hypothesis: Job growth has risen notably, particularly in technology and healthcare according to the recent economic report.\n",
            "True Entailment: True \n",
            "\n",
            "Probability that the hypothesis follows from the premise: 0.9973511695861816\n",
            "Probability that the hypothesis is a logical conclusion based on the premise: 0.9972361922264099\n",
            "Probability that sentence2 discusses the same event as sentence1: 0.9921026825904846\n",
            "Probability the premise and hypothesis are about the same topic: 0.9953747391700745 \n",
            "\n",
            "Premise: The Supreme Court ruled that the new voting regulations are unconstitutional, citing violations of the Fourteenth Amendment.\n",
            "Hypothesis: The Supreme Court found the new voting laws unconstitutional due to conflicts with the Fourteenth Amendment.\n",
            "True Entailment: True \n",
            "\n",
            "Probability that the hypothesis follows from the premise: 0.9960761666297913\n",
            "Probability that the hypothesis is a logical conclusion based on the premise: 0.9945541620254517\n",
            "Probability that sentence2 discusses the same event as sentence1: 0.9897037148475647\n",
            "Probability the premise and hypothesis are about the same topic: 0.9938703775405884 \n",
            "\n",
            "Premise: In his speech, the President emphasized the need for bipartisan cooperation to pass significant climate change legislation.\n",
            "Hypothesis: The President announced new healthcare policies aimed at reducing drug prices.\n",
            "True Entailment: False \n",
            "\n",
            "Probability that the hypothesis follows from the premise: 0.0018612506100907922\n",
            "Probability that the hypothesis is a logical conclusion based on the premise: 0.0009678666829131544\n",
            "Probability that sentence2 discusses the same event as sentence1: 0.006226825527846813\n",
            "Probability the premise and hypothesis are about the same topic: 0.0018192356219515204 \n",
            "\n",
            "Premise: The latest economic report shows a significant increase in job growth, especially in the technology and healthcare sectors.\n",
            "Hypothesis: The economic report indicates a downturn in the technology and healthcare sectors.\n",
            "True Entailment: False \n",
            "\n",
            "Probability that the hypothesis follows from the premise: 0.0018563636112958193\n",
            "Probability that the hypothesis is a logical conclusion based on the premise: 0.0016089021228253841\n",
            "Probability that sentence2 discusses the same event as sentence1: 0.005357588175684214\n",
            "Probability the premise and hypothesis are about the same topic: 0.003706967458128929 \n",
            "\n",
            "Premise: The Supreme Court ruled that the new voting regulations are unconstitutional, citing violations of the Fourteenth Amendment.\n",
            "Hypothesis: The Supreme Court upheld the new voting regulations as constitutional.\n",
            "True Entailment: False \n",
            "\n",
            "Probability that the hypothesis follows from the premise: 0.0309771616011858\n",
            "Probability that the hypothesis is a logical conclusion based on the premise: 0.043576546013355255\n",
            "Probability that sentence2 discusses the same event as sentence1: 0.10452988743782043\n",
            "Probability the premise and hypothesis are about the same topic: 0.06881490349769592 \n",
            "\n",
            "Complex and Subjective Question-Answer Pairs:\n",
            "Question: What did the President emphasize in his speech regarding climate change legislation?\n",
            "Answer: The President emphasized the need for bipartisan cooperation to pass significant climate change legislation.\n",
            "True QA Pair: True\n",
            "Probability that this is a valid question-answer pair: 0.9080022573471069 \n",
            "\n",
            "Probability that this answer correctly responds to the question: 0.9472517967224121 \n",
            "\n",
            "Question: What did the President emphasize in his speech regarding climate change legislation?\n",
            "Answer: He emphasized it was not so good for you.\n",
            "True QA Pair: False\n",
            "Probability that this is a valid question-answer pair: 0.1930142492055893 \n",
            "\n",
            "Probability that this answer correctly responds to the question: 0.3323969841003418 \n",
            "\n",
            "Question: Which sectors showed significant job growth in the latest economic report?\n",
            "Answer: The technology and healthcare sectors showed significant job growth in the latest economic report.\n",
            "True QA Pair: True\n",
            "Probability that this is a valid question-answer pair: 0.8759872317314148 \n",
            "\n",
            "Probability that this answer correctly responds to the question: 0.9131689071655273 \n",
            "\n",
            "Question: What was the Supreme Court's ruling on the new voting regulations?\n",
            "Answer: The Supreme Court ruled that the new voting regulations are unconstitutional, citing violations of the Fourteenth Amendment.\n",
            "True QA Pair: True\n",
            "Probability that this is a valid question-answer pair: 0.9332916140556335 \n",
            "\n",
            "Probability that this answer correctly responds to the question: 0.9507482647895813 \n",
            "\n",
            "Question: What did the President emphasize in his speech regarding climate change legislation?\n",
            "Answer: The President announced new healthcare policies aimed at reducing drug prices.\n",
            "True QA Pair: False\n",
            "Probability that this is a valid question-answer pair: 0.04266100376844406 \n",
            "\n",
            "Probability that this answer correctly responds to the question: 0.07874398678541183 \n",
            "\n",
            "Question: Which sectors showed significant job growth in the latest economic report?\n",
            "Answer: The economic report indicates a downturn in the technology and healthcare sectors.\n",
            "True QA Pair: False\n",
            "Probability that this is a valid question-answer pair: 0.05887255817651749 \n",
            "\n",
            "Probability that this answer correctly responds to the question: 0.03651316836476326 \n",
            "\n",
            "Question: What was the Supreme Court's ruling on the new voting regulations?\n",
            "Answer: The Supreme Court upheld the new voting regulations as constitutional.\n",
            "True QA Pair: False\n",
            "Probability that this is a valid question-answer pair: 0.9284882545471191 \n",
            "\n",
            "Probability that this answer correctly responds to the question: 0.9435800909996033 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "def get_score(model, tokenizer, input_ids):\n",
        "    pos_ids = tokenizer('Yes', return_tensors='pt').input_ids\n",
        "    neg_ids = tokenizer('No', return_tensors='pt').input_ids\n",
        "    pos_id = pos_ids[0, 0]\n",
        "    neg_id = neg_ids[0, 0]\n",
        "\n",
        "    logits = model(input_ids, decoder_input_ids=torch.zeros((input_ids.size(0), 1), dtype=torch.long).to(model.device)).logits\n",
        "    pos_logits = logits[:, 0, pos_id]\n",
        "    neg_logits = logits[:, 0, neg_id]\n",
        "    posneg_logits = torch.cat([pos_logits.unsqueeze(-1), neg_logits.unsqueeze(-1)], dim=1)\n",
        "    scores = torch.nn.functional.softmax(posneg_logits, dim=1)[:, 0]\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Complex and subjective entailment pairs\n",
        "entailment_pairs = [\n",
        "    (\"The new economic policy introduced by the government aims to stabilize the market. Experts believe it will have long-term benefits.\", \"Experts think the government's new economic policy will benefit the market in the long run.\", True),\n",
        "    (\"The actor gave an emotional speech at the awards ceremony, thanking his family and fans for their support.\", \"During the awards ceremony, the actor expressed gratitude towards his family and fans.\", True),\n",
        "    (\"The new study suggests a strong correlation between regular exercise and mental health improvements. It also points out the benefits of outdoor activities.\", \"The study indicates that regular exercise, especially outdoor activities, improves mental health.\", True),\n",
        "    (\"The new economic policy introduced by the government aims to stabilize the market. Experts believe it will have long-term benefits.\", \"The government's economic policy is expected to have short-term disadvantages.\", False),\n",
        "    (\"The actor gave an emotional speech at the awards ceremony, thanking his family and fans for their support.\", \"The actor criticized his family and fans during the awards ceremony.\", False),\n",
        "    (\"The new study suggests a strong correlation between regular exercise and mental health improvements. It also points out the benefits of outdoor activities.\", \"The study concludes that indoor activities have no impact on mental health.\", False)\n",
        "]\n",
        "\n",
        "# Complex and subjective question-answer pairs\n",
        "qa_pairs = [\n",
        "    (\"What is the expected impact of the government's new economic policy?\", \"Experts believe it will have long-term benefits.\", True),\n",
        "    (\"Who did the actor thank in his speech at the awards ceremony?\", \"He thanked his family and fans for their support.\", True),\n",
        "    (\"What does the new study suggest about regular exercise?\", \"It suggests a strong correlation between regular exercise and mental health improvements.\", True),\n",
        "    (\"What is the expected impact of the government's new economic policy?\", \"Experts believe it will have short-term disadvantages.\", False),\n",
        "    (\"Who did the actor thank in his speech at the awards ceremony?\", \"He criticized his family and fans for their support.\", False),\n",
        "    (\"What does the new study suggest about regular exercise?\", \"It concludes that indoor activities have no impact on mental health.\", False)\n",
        "]\n",
        "\n",
        "# Generate scores for entailment pairs\n",
        "print(\"Complex and Subjective Entailment Pairs:\")\n",
        "for premise, hypothesis, truth in entailment_pairs:\n",
        "    print(f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nTrue Entailment: {truth} \\n\")\n",
        "\n",
        "    prompt = f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nDoes the hypothesis follow from the premise?\\nAnswer:\"\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "    scores = get_score(model, tokenizer, input_ids)\n",
        "    print(f'Probability that the hypothesis follows from the premise: {scores[0].item()}')\n",
        "\n",
        "    prompt = f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nIs the hypothesis a logical conclusion based on the premise?\\nAnswer:\"\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "    scores = get_score(model, tokenizer, input_ids)\n",
        "    print(f'Probability that the hypothesis is a logical conclusion based on the premise: {scores[0].item()}')\n",
        "\n",
        "    prompt = f\"Sentence1: {premise}\\nSentence2: {hypothesis}\\nDo these sentences discuss the same event?\\nAnswer:\"\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "    scores = get_score(model, tokenizer, input_ids)\n",
        "    print(f'Probability that sentence2 discusses the same event as sentence1: {scores[0].item()}')\n",
        "\n",
        "    prompt_concrete = f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nAre the premise and hypothesis discussing the same topic?\\nAnswer:\"\n",
        "    input_ids = tokenizer(prompt_concrete, return_tensors='pt').input_ids\n",
        "    scores_concrete = get_score(model, tokenizer, input_ids)\n",
        "    print(f'Probability the premise and hypothesis are about the same topic: {scores_concrete[0].item()} \\n')\n",
        "\n",
        "# Generate scores for question-answer pairs\n",
        "print(\"Complex and Subjective Question-Answer Pairs:\")\n",
        "for question, answer, truth in qa_pairs:\n",
        "    print(f\"Question: {question}\\nAnswer: {answer}\\nTrue QA Pair: {truth}\")\n",
        "\n",
        "    # Check if it is a question-answer pair\n",
        "    prompt_qa = f\"Question: {question}\\nAnswer: {answer}\\nIs this a valid question-answer pair?\\nAnswer:\"\n",
        "    input_ids = tokenizer(prompt_qa, return_tensors='pt').input_ids\n",
        "    scores_qa = get_score(model, tokenizer, input_ids)\n",
        "    print(f'Probability that this is a valid question-answer pair: {scores_qa[0].item()} \\n')\n",
        "\n",
        "    # Check if the answer addresses the question\n",
        "    prompt = f\"Question: {question}\\nAnswer: {answer}\\nDoes this answer correctly respond to the question?\\nAnswer:\"\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "    scores_qa = get_score(model, tokenizer, input_ids)\n",
        "    print(f'Probability that this answer correctly responds to the question: {scores_qa[0].item()} \\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RTwqmTsIfO_",
        "outputId": "3f597e3a-1a79-433e-8d78-10487f8ca380"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complex and Subjective Entailment Pairs:\n",
            "Premise: The new economic policy introduced by the government aims to stabilize the market. Experts believe it will have long-term benefits.\n",
            "Hypothesis: Experts think the government's new economic policy will benefit the market in the long run.\n",
            "True Entailment: True \n",
            "\n",
            "Probability that the hypothesis follows from the premise: 0.9904779195785522\n",
            "Probability that the hypothesis is a logical conclusion based on the premise: 0.9885774850845337\n",
            "Probability that sentence2 discusses the same event as sentence1: 0.9891749620437622\n",
            "Probability the premise and hypothesis are about the same topic: 0.9898779392242432 \n",
            "\n",
            "Premise: The actor gave an emotional speech at the awards ceremony, thanking his family and fans for their support.\n",
            "Hypothesis: During the awards ceremony, the actor expressed gratitude towards his family and fans.\n",
            "True Entailment: True \n",
            "\n",
            "Probability that the hypothesis follows from the premise: 0.9933193922042847\n",
            "Probability that the hypothesis is a logical conclusion based on the premise: 0.9931649565696716\n",
            "Probability that sentence2 discusses the same event as sentence1: 0.9887447357177734\n",
            "Probability the premise and hypothesis are about the same topic: 0.9909452199935913 \n",
            "\n",
            "Premise: The new study suggests a strong correlation between regular exercise and mental health improvements. It also points out the benefits of outdoor activities.\n",
            "Hypothesis: The study indicates that regular exercise, especially outdoor activities, improves mental health.\n",
            "True Entailment: True \n",
            "\n",
            "Probability that the hypothesis follows from the premise: 0.9857812523841858\n",
            "Probability that the hypothesis is a logical conclusion based on the premise: 0.984031081199646\n",
            "Probability that sentence2 discusses the same event as sentence1: 0.9716839790344238\n",
            "Probability the premise and hypothesis are about the same topic: 0.9847351312637329 \n",
            "\n",
            "Premise: The new economic policy introduced by the government aims to stabilize the market. Experts believe it will have long-term benefits.\n",
            "Hypothesis: The government's economic policy is expected to have short-term disadvantages.\n",
            "True Entailment: False \n",
            "\n",
            "Probability that the hypothesis follows from the premise: 0.04101436585187912\n",
            "Probability that the hypothesis is a logical conclusion based on the premise: 0.03158596158027649\n",
            "Probability that sentence2 discusses the same event as sentence1: 0.12739214301109314\n",
            "Probability the premise and hypothesis are about the same topic: 0.06066853925585747 \n",
            "\n",
            "Premise: The actor gave an emotional speech at the awards ceremony, thanking his family and fans for their support.\n",
            "Hypothesis: The actor criticized his family and fans during the awards ceremony.\n",
            "True Entailment: False \n",
            "\n",
            "Probability that the hypothesis follows from the premise: 0.001751019386574626\n",
            "Probability that the hypothesis is a logical conclusion based on the premise: 0.0012119758175686002\n",
            "Probability that sentence2 discusses the same event as sentence1: 0.0027404504362493753\n",
            "Probability the premise and hypothesis are about the same topic: 0.002645291155204177 \n",
            "\n",
            "Premise: The new study suggests a strong correlation between regular exercise and mental health improvements. It also points out the benefits of outdoor activities.\n",
            "Hypothesis: The study concludes that indoor activities have no impact on mental health.\n",
            "True Entailment: False \n",
            "\n",
            "Probability that the hypothesis follows from the premise: 0.029449164867401123\n",
            "Probability that the hypothesis is a logical conclusion based on the premise: 0.03394142538309097\n",
            "Probability that sentence2 discusses the same event as sentence1: 0.07404205948114395\n",
            "Probability the premise and hypothesis are about the same topic: 0.06084921583533287 \n",
            "\n",
            "Complex and Subjective Question-Answer Pairs:\n",
            "Question: What is the expected impact of the government's new economic policy?\n",
            "Answer: Experts believe it will have long-term benefits.\n",
            "True QA Pair: True\n",
            "Probability that this is a valid question-answer pair: 0.688636302947998 \n",
            "\n",
            "Probability that this answer correctly responds to the question: 0.7908161878585815 \n",
            "\n",
            "Question: Who did the actor thank in his speech at the awards ceremony?\n",
            "Answer: He thanked his family and fans for their support.\n",
            "True QA Pair: True\n",
            "Probability that this is a valid question-answer pair: 0.5760638117790222 \n",
            "\n",
            "Probability that this answer correctly responds to the question: 0.7286789417266846 \n",
            "\n",
            "Question: What does the new study suggest about regular exercise?\n",
            "Answer: It suggests a strong correlation between regular exercise and mental health improvements.\n",
            "True QA Pair: True\n",
            "Probability that this is a valid question-answer pair: 0.8172323107719421 \n",
            "\n",
            "Probability that this answer correctly responds to the question: 0.8728597164154053 \n",
            "\n",
            "Question: What is the expected impact of the government's new economic policy?\n",
            "Answer: Experts believe it will have short-term disadvantages.\n",
            "True QA Pair: False\n",
            "Probability that this is a valid question-answer pair: 0.523218035697937 \n",
            "\n",
            "Probability that this answer correctly responds to the question: 0.7884467840194702 \n",
            "\n",
            "Question: Who did the actor thank in his speech at the awards ceremony?\n",
            "Answer: He criticized his family and fans for their support.\n",
            "True QA Pair: False\n",
            "Probability that this is a valid question-answer pair: 0.013977497816085815 \n",
            "\n",
            "Probability that this answer correctly responds to the question: 0.019763363525271416 \n",
            "\n",
            "Question: What does the new study suggest about regular exercise?\n",
            "Answer: It concludes that indoor activities have no impact on mental health.\n",
            "True QA Pair: False\n",
            "Probability that this is a valid question-answer pair: 0.2812807559967041 \n",
            "\n",
            "Probability that this answer correctly responds to the question: 0.40631410479545593 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}